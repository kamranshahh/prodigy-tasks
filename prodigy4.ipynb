{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObyWO-UoqMgR",
        "outputId": "40e7b56c-92a9-48ba-9997-805dde42f65f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 62ms/step - accuracy: 0.8294 - loss: 0.5429 - val_accuracy: 0.9809 - val_loss: 0.0647\n",
            "Epoch 2/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 60ms/step - accuracy: 0.9709 - loss: 0.0975 - val_accuracy: 0.9849 - val_loss: 0.0483\n",
            "Epoch 3/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 61ms/step - accuracy: 0.9795 - loss: 0.0693 - val_accuracy: 0.9868 - val_loss: 0.0466\n",
            "Epoch 4/10\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Load the Sign Language MNIST dataset (for demonstration purposes)\n",
        "# You can replace this with any hand gesture dataset you have\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()  # Replace this with the actual hand gesture dataset\n",
        "\n",
        "# Resize images to match the input size expected by CNN (28x28 for MNIST)\n",
        "X_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "X_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "\n",
        "# Convert labels to categorical (one-hot encoding)\n",
        "y_train = to_categorical(y_train, 10)  # Adjust based on the number of gestures\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')  # Adjust this number based on the number of gestures\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=64)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {test_acc}\")\n",
        "\n",
        "# Generate classification report\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true_classes, y_pred_classes))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Real-Time Gesture Recognition using OpenCV\n",
        "cap = cv2.VideoCapture(0)  # Open webcam\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Preprocess the frame: Convert to grayscale, resize, and normalize\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    resized = cv2.resize(gray, (28, 28))\n",
        "    normalized = resized.astype('float32') / 255.0\n",
        "    reshaped = normalized.reshape(1, 28, 28, 1)\n",
        "\n",
        "    # Predict the gesture\n",
        "    prediction = model.predict(reshaped)\n",
        "    predicted_class = np.argmax(prediction)\n",
        "\n",
        "    # Display the prediction on the frame\n",
        "    cv2.putText(frame, f\"Predicted Gesture: {predicted_class}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
        "\n",
        "    # Show the frame\n",
        "    cv2.imshow('Hand Gesture Recognition', frame)\n",
        "\n",
        "    # Exit on pressing 'q'\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    }
  ]
}